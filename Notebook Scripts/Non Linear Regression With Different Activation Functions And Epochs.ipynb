{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ns non-linear-regression\n",
    "  (:require\n",
    "      [clojure.set :as set]\n",
    "      [clojure.string :as str]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/deriv"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";;; Functions related to symbolic differentiation.\n",
    "\n",
    "(defn variable? [e] (symbol? e))\n",
    "\n",
    ";Should be a symbol first. And should be equal. \n",
    "\n",
    "(defn same-variable? [v1 v2] \n",
    " (and (variable? v1) (variable? v2) (= v1 v2)))\n",
    "\n",
    ";Equality check of two numbers\n",
    "\n",
    "(defn =number? [x n] \n",
    " (and (number? x) (= x n)))\n",
    "\n",
    "(defn make-sum [a1 a2]\n",
    " (cond (=number? a1 0) a2\n",
    "       (=number? a2 0) a1\n",
    "       (and (number? a1) (number? a2)) (+ a1 a2)\n",
    "       :else (list '+ a1 a2)))\n",
    "\n",
    "(defn make-product [m1 m2]\n",
    " (cond (or (=number? m1 0) (=number? m2 0)) 0\n",
    "       (=number? m1 1) m2\n",
    "       (=number? m2 1) m1\n",
    "       (and (number? m1) (number? m2)) (* m1 m2)\n",
    "       :else (list '* m1 m2)))\n",
    "\n",
    "(defn sum? [x]\n",
    " (and (seq? x) (= (first x) '+)))\n",
    "\n",
    ";To get the first part of sum expression.\n",
    "\n",
    "(defn addend [s] \n",
    " (second s))\n",
    "\n",
    "(defn augend [s]\n",
    " (last s))\n",
    "\n",
    "(defn minus? [x]\n",
    " (and (seq? x) (= (first x) '-)))\n",
    "\n",
    "(defn substract [a1 a2]\n",
    " (cond (=number? a1 0) (list '* '-1 a2)\n",
    "       (=number? a2 0) a1\n",
    "       (and (number? a1) (number? a2)) (- a1 a2)\n",
    "       :else (list '- a1 a2)))\n",
    "\n",
    "(defn product? [x]\n",
    " (and (seq? x) (= (first x) '*)))\n",
    "\n",
    "(defn multiplier [p]\n",
    " (second p))\n",
    "\n",
    "(defn multiplicand [p]\n",
    "  (last p))\n",
    "\n",
    ";Now to make exponential. \n",
    "\n",
    "(defn make-exponentiation [b e]\n",
    " (cond (=number? e 0) 1\n",
    "       (=number? e 1) b\n",
    "       (and (number? b) (number? e)) (Math/pow b e)\n",
    "       :else (list '** b e)))\n",
    "\n",
    "(defn exponentiation? [exp]\n",
    "  (and (seq? exp) (= '** (first exp))))\n",
    "\n",
    "(defn base [exp]\n",
    "  (second exp))\n",
    "\n",
    "(defn exponent [exp]\n",
    "  (last exp))\n",
    "\n",
    "(def trigonometry #{'sin 'cos 'tan 'exp 'log 'sig 'tanh})\n",
    "\n",
    "(defn trigonometry? [exp]\n",
    "    (contains? trigonometry (first exp)))\n",
    "\n",
    "(defn deriv [exp var]  \n",
    "\n",
    " (cond (number? exp) 0 \n",
    "       (variable? exp) (if (same-variable? exp var) 1 0)\n",
    "     \n",
    "       (sum? exp)\n",
    "       (make-sum (deriv (addend exp) var)\n",
    "          (deriv (augend exp) var))\n",
    "       \n",
    "       (minus? exp)\n",
    "       (substract (deriv (addend exp) var)\n",
    "          (list '- (deriv (augend exp) var)))\n",
    "       \n",
    "      (product? exp)\n",
    "       (make-sum\n",
    "        (make-product (multiplier exp)\n",
    "             (deriv (multiplicand exp) var))\n",
    "            (make-product (deriv (multiplier exp) var)\n",
    "               (multiplicand exp)))\n",
    "       \n",
    "       (exponentiation? exp)\n",
    "         (make-product \n",
    "          (make-product (deriv (base exp) var) (exponent exp))\n",
    "          (make-exponentiation (base exp) (dec (exponent exp)))) \n",
    "       \n",
    "       (trigonometry? exp)\n",
    "          (let [rest-exp (if (= (count (rest exp)) 1) (first (rest exp)) (rest exp))]\n",
    "                 (cond (= (first exp) 'sin) (list '* (list 'cos rest-exp) (deriv rest-exp var))\n",
    "                       (= (first exp) 'cos) (list '* (- 1) (list 'sin rest-exp) (deriv rest-exp var))\n",
    "                       (= (first exp) 'tan) (list '* (list '** (list 'sec rest-exp) 2) (deriv rest-exp var))\n",
    "                       (= (first exp) 'exp) (list '* (list 'exp rest-exp) (deriv rest-exp var))\n",
    "                       (= (first exp) 'log) (list '* (list '/ 1 rest-exp) (deriv rest-exp var))\n",
    "                       (= (first exp) 'sig) (list '* (list 'sig rest-exp) (list '- 1 (list 'sig rest-exp)) (deriv rest-exp var))\n",
    "                       (= (first exp) 'tanh) (list '* (list '- 1 (list '** (list 'tanh rest-exp) 2)) (deriv rest-exp var))\n",
    "                       (= (first exp) 'relu) (prn \"At least print something dude.\")\n",
    "                      \n",
    "                       ))\n",
    "      :else\n",
    "     (prn \"Invalid Operation\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    ";;; My understand of the problem. (To be discussed with Suresh dai)\n",
    "\n",
    ";;; We start with a vector of input features. (x1 x2 x3 x4 .... xn)\n",
    "\n",
    ";;; We then choose a matrix of coefficients that we want to learn. It will be of the size n times k.\n",
    ";;; k is the parameter we choose.\n",
    "\n",
    ";;; We could think of one matrix m as one layer in Neural Network,\n",
    ";;; and each columns as neurons with weights (m1 m2 m3 m4), (m5 m6 m7 m8), and so on.\n",
    "\n",
    ";;; So, in a way, we are selecting the number of neurons in a layer with the value of k.\n",
    "\n",
    ";;; The output of each neuron is a scalar (after the dot product of its weights with feature vector.)\n",
    "\n",
    ";;; We then activate this scalar output depending on our activation function. \n",
    "\n",
    ";;; So, if we have 5 neurons in one layer, (that is k = 5), we will have 5 outputs after activation in each neurons.\n",
    "\n",
    ";;; After a series of these operations in each layers (which is usually deep and hence the name deep learning),\n",
    ";;; We finally multiply the output with a final-layer-vector to get our scalar output y. \n",
    ";;; Our final-layer will be a vector of the form (n1 n2 ......nk).\n",
    "\n",
    ";;; The size of the final-layer depends on the size of the output of the preceding layer (that is penultimate layer).\n",
    "\n",
    ";;; And hence, we obtain our scalar predicted y using feature vector (x1 x2 ...xn) with the help of matrix coefficients\n",
    ";;; m and final-layer-vector n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/j"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(def j 0.000001) ; Defining the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/**"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";;; Defining some mathematica operations that will be used later. \n",
    "\n",
    "(defn tanh [x]\n",
    "    (Math/tanh x))\n",
    "(defn sig [x]\n",
    "    (/ 1 (+ 1 (Math/exp (- x)))))\n",
    "(defn ** [x y]\n",
    "    (Math/pow x y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/min-max"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";Creating a min-max function which is used inside normalization function below. \n",
    ";num -min / max - min \n",
    "\n",
    "(defn min-max [v]\n",
    "    (let [min-num (apply min v)\n",
    "          max-num (apply max v)\n",
    "          ]\n",
    "        (map #(float (/ (- % min-num) (- max-num min-num))) v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/transpose"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";Creating a function that calculates the tranpose of a matrix. \n",
    "\n",
    "(defn transpose [m]\n",
    "    (apply map vector m)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/normalize"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";Creating a function that normalizes the data using minimum and maximum value.\n",
    "\n",
    "(defn normalize [data]\n",
    "    (transpose (map #(min-max %) (transpose data)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/create-epochs"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";Creating a function that cre different epochs of the data. \n",
    ";Concats data depending on the value of n.\n",
    "\n",
    "(defn create-epochs [data n]\n",
    "    (if (> n 1) (create-epochs (concat data data) (dec n))\n",
    "        data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/create-features"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";Creating a function that creates symbolic features depending on the value of size. \n",
    "\n",
    "(defn create-features [size]\n",
    "    (map read-string (map #(str \"x\" %) (range 1 (+ size 1))))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(x1 x2 x3 x4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";For example, if the size of our input features is 4, then\n",
    "(create-features 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/create-symbolic-matrix"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";Creating a function that creates a symbolic matrix depending on the value of row and column.\n",
    "\n",
    "(defn create-symbolic-matrix [row column]\n",
    "    (partition row (map read-string (map #(str \"m\" %) (range 1 (+ (* row column) 1)))))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((m1 m2 m3 m4) (m5 m6 m7 m8) (m9 m10 m11 m12) (m13 m14 m15 m16) (m17 m18 m19 m20))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; For example, if size of our matrix is 4 * 5\n",
    "(create-symbolic-matrix 4 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/create-final-layer"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "; Creating a function that creates a final layer depending on the value of size.\n",
    "; Value of size is decided depending on the output of the preceding layer. \n",
    "\n",
    "(defn create-final-layer [size]\n",
    "     (map read-string (map #(str \"n\" %) (range 1 (+ size 1))))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(n1 n2 n3 n4 n5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";For example, if our final layer is of size 5 * 1, \n",
    ";it means we are getting 5 outputs out of our final layer (after multiplication and activation). \n",
    "\n",
    "(create-final-layer 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/assign-final-layer"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";# Creating three functions that assigns values to the features, coefficients matrix and final layer vector.\n",
    "\n",
    "(defn assign-features [n features]\n",
    "     (map #(str \"(def x\" %1 \" \" %2 \")\") (range 1 (+ n 1)) features)\n",
    "    )\n",
    "\n",
    "(defn assign-coefficients [n coefficients]\n",
    "     (map #(str \"(def m\" %1 \" \" %2 \")\") (range 1 (+ n 1)) coefficients)\n",
    "    )\n",
    "\n",
    "(defn assign-final-layer [n final-layer]\n",
    "     (map #(str \"(def n\" %1 \" \" %2 \")\") (range 1 (+ n 1)) final-layer)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/dot-symbol"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";Creating a function that calculates symbolic dot product or numeric dot product depending on the arguments provided.\n",
    "\n",
    "(defn dot-symbol [v1 v2]\n",
    "    (if (or (symbol? (first v1)) (symbol? (first v2)))\n",
    "        (conj (map #(list '* %1 %2)  v1 v2) '+)\n",
    "        (reduce + (map * v1 v2))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((+ (* x1 m1) (* x2 m2) (* x3 m3) (* x4 m4)) (+ (* x1 m5) (* x2 m6) (* x3 m7) (* x4 m8)) (+ (* x1 m9) (* x2 m10) (* x3 m11) (* x4 m12)) (+ (* x1 m13) (* x2 m14) (* x3 m15) (* x4 m16)) (+ (* x1 m17) (* x2 m18) (* x3 m19) (* x4 m20)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";This is an important operation. \n",
    ";We are calculating X times M here. \n",
    "\n",
    "(map #(dot-symbol (create-features 4) %)  (create-symbolic-matrix 4 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/multiply-and-activate"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";Creating a function that first calculates the dot product and then activates the output. \n",
    ";That is, X * M operation followed by activation. \n",
    "\n",
    ";Takes the input feature, coefficients matrix and activation-function as the arguments.\n",
    "\n",
    "(defn multiply-and-activate [feature matrix activation-function]\n",
    "    (let [multiply (map #(dot-symbol feature %) matrix)\n",
    "          activate  (map #(read-string (str \"(\" activation-function, \" \", %, \")\")) multiply)]\n",
    "        activate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/final-layer-multiplication"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";Creating a function that does final layer multiplication to give a scalar output y. \n",
    "\n",
    "(defn final-layer-multiplication [activated-output final-layer]\n",
    "    (cons '+ (map #(read-string (str \"( * \", %2, \" \", %1, \")\")) activated-output final-layer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/non-linear-regression"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";Creating a function that performs non-linear-regression on the given data. \n",
    ";Takes the data, hidden parameter k (column of matrix m that we want) and activation function as arguments.\n",
    ";Sigmoid and Tanh activation functions are implemented inside this function for now. \n",
    "\n",
    "(defn non-linear-regression [data k activation-function]\n",
    "\n",
    "    ;Initalizing m (which is a matrix) and n (which is a vector of coefficients in final layer) with 0.5\n",
    "    ;Fixed value of 0.5 instead of randomly generated values is used, only so as to compare out results for different\n",
    "    ;activation functions, different value of k, and different number of epochs. \n",
    "    \n",
    "    (def m (vec (repeat (* k (count (rest (first data)) )) 0.5))) \n",
    "    (def n (vec (repeat k 0.5))) \n",
    "    ;Initializing error as 0 at the begining. \n",
    "    (def error 0)\n",
    "    \n",
    "    ;Running a loop across the whole data\n",
    "    (loop [d  data]\n",
    "        (when (> (count  d) 1)\n",
    "            \n",
    "            \n",
    "            (do (def y (first (first d)));First element of the list is considered as y. \n",
    "                (def x (rest (first d)));Rest of the elements of the list is considered as x. (x is a vector)\n",
    "                \n",
    "                ;Creates x-features (x1 x2 ..xn) depending on the count of data.\n",
    "                (def x-features (create-features (count (rest (first data)))) )\n",
    "              \n",
    "                ;Creates m-coefficients matrix ((m1 m2) (m3 m4) ...mn) depending on the count of data.\n",
    "                (def m-matrix (create-symbolic-matrix (count (rest (first data))) k))\n",
    "                \n",
    "                ;Creates a final-layer vector depending on the value of k\n",
    "                (def final-layer (create-final-layer k))\n",
    "                \n",
    "                ;Predicts y after performing 3 operations in series.\n",
    "                ;First multiplication of x and m, followed by activation and again mulplication with final layer n.\n",
    "                (def magical-predicted-y (final-layer-multiplication (multiply-and-activate x-features \n",
    "                                            m-matrix activation-function) final-layer))\n",
    "                \n",
    "                ;Calculates the loss function. \n",
    "                (def loss-function (read-string (str \"(** (- y \", magical-predicted-y, \") 2)\")))\n",
    "                \n",
    "                \n",
    "                ;Calculates the derivate of loss function by both matrix of coefficients and final-layer-vector.\n",
    "                ;I have used map operation and anonymous function to calculate derivate by vector and matrix.\n",
    "                (def deriv-expressions-matrix (map #(deriv loss-function %) (flatten m-matrix)))\n",
    "                (def deriv-expressions-n (map #(deriv loss-function %) final-layer))\n",
    "                \n",
    "                    \n",
    "                ;Assigns the calculated value of x, m and n (vectors and matrix) to the symbols obtained above.\n",
    "                ;Again using the map operation to loop over a vector and matrix.\n",
    "                \n",
    "                ;eval operation evaluates the value of the symbolic expression. \n",
    "                \n",
    "                ;Also, we need to perform the side effect inside this map operation as map produces lazy sequence only.\n",
    "                \n",
    "                \n",
    "                ;Therefore, we use dorun operation here, which performs side effect in clojure map operation. \n",
    "                (dorun (map eval (map read-string (assign-features (count x) x))))\n",
    "                (dorun (map eval (map read-string (assign-coefficients (count m) m))))\n",
    "                (dorun (map eval (map read-string (assign-final-layer (count final-layer) n))))\n",
    "                \n",
    "                \n",
    "                \n",
    "                ;Using eval operation again to evaluate the values\n",
    "                (def ans-matrix (map eval deriv-expressions-matrix))\n",
    "                (def ans-n (map eval deriv-expressions-n))\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                ;Final calculation of m matrix, which is our coefficient matrix and final-layer vector n.\n",
    "                ;This calculated value of m and n  is updated in every iteration. \n",
    "                (def m (map + (map #(* (- j) %) ans-matrix) m))\n",
    "                (def n (map + (map #(* (- j) %) ans-n) n))\n",
    "                \n",
    "                ;Also calculating and updating our error value.\n",
    "                (def error-evaluated (eval loss-function))\n",
    "                (def error (+ error error-evaluated))\n",
    "            )\n",
    "            (recur (rest d))))\n",
    "    (do\n",
    "        (prn \"The final calculated coefficient matrix is:\" (partition (count (rest (first data))) m))\n",
    "        (prn \"The final calculated last layer vector is: \" n)\n",
    "        (prn \"The final calculated error is: \" error)\n",
    "        )\n",
    "     \n",
    "         ; Returning the final value of m, n, and error. \n",
    "         ; m is a matrix of coefficients, n a vector of coefficients and error a scalar value. \n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#'non-linear-regression/mtcars-data"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ";;; Use this in case you have problem loading csv data using fressian. \n",
    "\n",
    ";;; Using Motorcars dataset (popular dataset used in data analysis) to perform our non-linear-regression function.\n",
    "\n",
    ";;; The first entry is mpg (miles per gallon) which we will be using as our output variable y.\n",
    ";;; Rest of the entries (disp, hp, drat and wt) we will be using as our features. \n",
    "\n",
    "(def mtcars-data (drop 1 '([mpg disp hp drat wt] [21 160 110 3.9 2.62] [21 160 110 3.9 2.875] [22.8 108 93 3.85 2.32] [21.4 258 110 3.08 3.215] [18.7 360 175 3.15 3.44] [18.1 225 105 2.76 3.46] [14.3 360 245 3.21 3.57] [24.4 146.7 62 3.69 3.19] [22.8 140.8 95 3.92 3.15] [19.2 167.6 123 3.92 3.44] [17.8 167.6 123 3.92 3.44] [16.4 275.8 180 3.07 4.07] [17.3 275.8 180 3.07 3.73] [15.2 275.8 180 3.07 3.78] [10.4 472 205 2.93 5.25] [10.4 460 215 3 5.424] [14.7 440 230 3.23 5.345] [32.4 78.7 66 4.08 2.2] [30.4 75.7 52 4.93 1.615] [33.9 71.1 65 4.22 1.835] [21.5 120.1 97 3.7 2.465] [15.5 318 150 2.76 3.52] [15.2 304 150 3.15 3.435] [13.3 350 245 3.73 3.84] [19.2 400 175 3.08 3.845] [27.3 79 66 4.08 1.935] [26 120.3 91 4.43 2.14] [30.4 95.1 113 3.77 1.513] [15.8 351 264 4.22 3.17] [19.7 145 175 3.62 2.77] [15 301 335 3.54 3.57] [21.4 121 109 4.11 2.78])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([21 160 110 3.9 2.62] [21 160 110 3.9 2.875] [22.8 108 93 3.85 2.32] [21.4 258 110 3.08 3.215] [18.7 360 175 3.15 3.44] [18.1 225 105 2.76 3.46] [14.3 360 245 3.21 3.57] [24.4 146.7 62 3.69 3.19] [22.8 140.8 95 3.92 3.15] [19.2 167.6 123 3.92 3.44] [17.8 167.6 123 3.92 3.44] [16.4 275.8 180 3.07 4.07] [17.3 275.8 180 3.07 3.73] [15.2 275.8 180 3.07 3.78] [10.4 472 205 2.93 5.25] [10.4 460 215 3 5.424] [14.7 440 230 3.23 5.345] [32.4 78.7 66 4.08 2.2] [30.4 75.7 52 4.93 1.615] [33.9 71.1 65 4.22 1.835] [21.5 120.1 97 3.7 2.465] [15.5 318 150 2.76 3.52] [15.2 304 150 3.15 3.435] [13.3 350 245 3.73 3.84] [19.2 400 175 3.08 3.845] [27.3 79 66 4.08 1.935] [26 120.3 91 4.43 2.14] [30.4 95.1 113 3.77 1.513] [15.8 351 264 4.22 3.17] [19.7 145 175 3.62 2.77] [15 301 335 3.54 3.57] [21.4 121 109 4.11 2.78])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtcars-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The final calculated coefficient matrix is:\" ((0.5000179362045275 0.5 0.5 0.5000193728587182) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5000179362045275 0.5 0.5 0.5000193728587182))\n",
      "\"The final calculated last layer vector is: \" (0.5001184163925638 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5001184163925638)\n",
      "\"The final calculated error is: \" 258.64353062043114\n"
     ]
    }
   ],
   "source": [
    ";Peforming non-linear-regression on our data after normalizing.\n",
    "; Activation function chosen is tanh.\n",
    ";Value of k is given 10. \n",
    "(non-linear-regression (normalize mtcars-data) 10 'tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The final calculated coefficient matrix is:\" ((0.5000084754653766 0.5 0.5 0.5000089903688086) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5000084754653766 0.5 0.5 0.5000089903688086))\n",
      "\"The final calculated last layer vector is: \" (0.5001310490002991 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5001310490002991)\n",
      "\"The final calculated error is: \" 288.4312402356455\n"
     ]
    }
   ],
   "source": [
    ";With sigmoid activation and rest of the arguments same for error comparison. \n",
    "(non-linear-regression (normalize mtcars-data) 10 'sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The final calculated coefficient matrix is:\" ((0.5000357631427715 0.5 0.5 0.500039110333436) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5000357631427715 0.5 0.5 0.500039110333436))\n",
      "\"The final calculated last layer vector is: \" (0.5002352306963533 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5002352306963533)\n",
      "\"The final calculated error is: \" 522.803984383881\n"
     ]
    }
   ],
   "source": [
    ";With same parameters as earlier with tanh activation but with different number of epochs (chosen as 2 here).\n",
    "(non-linear-regression (create-epochs (normalize mtcars-data) 2) 10 'tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The final calculated coefficient matrix is:\" ((0.5000168049192577 0.5 0.5 0.5000180310082089) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5000168049192577 0.5 0.5 0.5000180310082089))\n",
      "\"The final calculated last layer vector is: \" (0.5002637203345409 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5002637203345409)\n",
      "\"The final calculated error is: \" 584.7443224139599\n"
     ]
    }
   ],
   "source": [
    ";With same parameters as above for tanh but with sigmoid activation. (With number of epochs also 2).=\n",
    "(non-linear-regression (create-epochs (normalize mtcars-data) 2) 10 'sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    ";;; As we can see here, tanh activation is giving us less error than sigmoid function in our dataset here,\n",
    ";;; in both our cases (that is different number of epochs). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    ";;; Now, let us compare out results with different number of k. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The final calculated coefficient matrix is:\" ((0.500008144644854 0.5 0.5 0.500008554081008) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.500008144644854 0.5 0.5 0.500008554081008))\n",
      "\"The final calculated last layer vector is: \" (0.5000524800675661 0.5 0.5 0.5 0.5000524800675661)\n",
      "\"The final calculated error is: \" 53.07957886305005\n"
     ]
    }
   ],
   "source": [
    ";Peforming non-linear-regression on our data after normalizing.\n",
    "; Activation function chosen is tanh.\n",
    ";Value of k is given 5. \n",
    "(non-linear-regression (normalize mtcars-data) 5 'tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The final calculated coefficient matrix is:\" ((0.50000391434051 0.5 0.5 0.5000040835566656) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.5 0.5 0.5 0.5) (0.50000391434051 0.5 0.5 0.5000040835566656))\n",
      "\"The final calculated last layer vector is: \" (0.5000573984471849 0.5 0.5 0.5 0.5000573984471849)\n",
      "\"The final calculated error is: \" 56.88449765506259\n"
     ]
    }
   ],
   "source": [
    ";With sigmoid activation and rest of the arguments same for error comparison. \n",
    "(non-linear-regression (normalize mtcars-data) 5 'sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    ";;; As we see, our error is hugely reduced in both tanh and sigmoid activation after we have reduce the size of k.\n",
    "\n",
    ";;; So maybe, for our small dataset, less number of neurons in a layer (that is less value of k) actually \n",
    ";;; peforms better. (This to be discussed and confirmed with Suresh Sir.)\n",
    "\n",
    ";;; And as above, even for less value of k, tanh activation is giving less error than sigmoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    ";;; Now, we will compare the result that we will get for k = 1.\n",
    ";;; k =1 is equivalent of using a single neuron (that is a vector instead of a matrix) in our first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The final calculated coefficient matrix is:\" ((0.5000006241324676 0.5 0.5 0.49999979952074447))\n",
      "\"The final calculated last layer vector is: \" (0.4999994626532884)\n",
      "\"The final calculated error is: \" 3.645511191012884\n"
     ]
    }
   ],
   "source": [
    ";Peforming non-linear-regression on our data after normalizing.\n",
    "; Activation function chosen is tanh.\n",
    ";Value of k is given 1. \n",
    "(non-linear-regression (normalize mtcars-data) 1 'tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The final calculated coefficient matrix is:\" ((0.5000005317723432 0.5 0.5 0.5000003171125073))\n",
      "\"The final calculated last layer vector is: \" (0.4999969561442015)\n",
      "\"The final calculated error is: \" 2.6323224896648707\n"
     ]
    }
   ],
   "source": [
    ";With sigmoid activation and rest of the arguments same for error comparison. \n",
    "(non-linear-regression (normalize mtcars-data) 1 'sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ";(linear-regression-multiple (normalize (drop 1 mtcars-data))\n",
    "\n",
    ";\"The final calculated coefficient vector is: \" (0.4999839215052779 0.4999867851580205 0.49999523565657705 0.49998443109115)\n",
    ";\"The final error is: \" 13.053241688148475\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ";##### IMPORTANT STUFF ##### COMPARISION BETWEEN LINEAR AND NON LINEAR ######; \n",
    "\n",
    "; As we see, error is more in linear using the same normalized data for same number of epochs that is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    ";For k = 1, the error is again hugely reduced. \n",
    "\n",
    ";Also, as a contrast from earlier comparisons, sigmoid activation is giving less error than tanh activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The final calculated coefficient matrix is:\" ((0.5000075610356552 0.5 0.5 0.49999346881499274))\n",
      "\"The final calculated last layer vector is: \" (0.4999678721489914)\n",
      "\"The final calculated error is: \" 58.85304053854204\n"
     ]
    }
   ],
   "source": [
    ";Now, one last exploration.\n",
    ";For k = 1 (that is a vector), we increase the number of epochs 5 times. \n",
    "(non-linear-regression (create-epochs (normalize mtcars-data) 5) 1 'tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The final calculated coefficient matrix is:\" ((0.5000076199201023 0.5 0.5 0.5000039890583096))\n",
      "\"The final calculated last layer vector is: \" (0.49993193693385934)\n",
      "\"The final calculated error is: \" 42.41720756353565\n"
     ]
    }
   ],
   "source": [
    "(non-linear-regression (create-epochs (normalize mtcars-data) 5) 1 'sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    ";Things to learn and ask Suresh dai\n",
    "\n",
    ";How to know the saturation point of coefficients that we are learning. (That is when to stop the updating process.)\n",
    ";Other activation functions too and their implementations in different cases. \n",
    "\n",
    ";After learning more about that, I will implement that in our non-linear-regression function here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lein-Clojure",
   "language": "clojure",
   "name": "lein-clojure"
  },
  "language_info": {
   "file_extension": ".clj",
   "mimetype": "text/x-clojure",
   "name": "clojure",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
